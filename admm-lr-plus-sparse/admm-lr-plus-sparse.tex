\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage{csquotes}             % For biblatex
\usepackage{lmodern}             
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{epsfig}
\usepackage{auto-pst-pdf}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-grad}
\usepackage{pst-sigsys}
\usepackage{pstricks-add}
\usepackage{ifthen}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}           
%\usepackage{wrapfig}
%\usepackage{paralist}           % Make lists into paragraphs
\usepackage[style=ieee]{biblatex}
\usepackage{xpatch}
\usepackage{tikz-cd}
\usepackage[framemethod=PSTricks]{mdframed}

\usepackage[a4paper,
top=1.2in,
bottom=1.5in,
left=1.0in,
right=1.0in]{geometry}

% Avoid hyperref problems
\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\texttt#1{<#1>}%
  \def\cite#1{<#1>}%
  \def\eqref#1{<#1>}%
}

% ------------------------------------------------------------------------
% Colors
\definecolor{red}{RGB}{153,0,0}
\definecolor{green}{RGB}{0,153,0}			
\definecolor{blue}{RGB}{0,0,153} 
\definecolor{darkred}{RGB}{90,0,0}
\definecolor{darkgreen}{RGB}{0,90,0}
\definecolor{darkblue}{RGB}{0,0,90}			

\hypersetup{colorlinks,urlcolor=darkred,linkcolor=darkred,citecolor=green}
% ------------------------------------------------------------------------


% ------------------------------------------------------------------------
% Environments

\allowdisplaybreaks


\declaretheoremstyle[spaceabove=6pt, spacebelow=6pt,
headfont=\normalfont\bfseries,
notefont=\mdseries, notebraces={(}{)},
bodyfont=\normalfont,
postheadspace=1em, 
mdframed={
        backgroundcolor=black!10, 
            linecolor=black!10, 
            innertopmargin=0.3cm,
            roundcorner=5pt, 
            innerbottommargin=0.5cm, 
            skipabove=\parsep, 
            skipbelow=\parsep
          }
]{mystyle}
\declaretheorem[numberwithin=section,style=mystyle]{Theorem}
\declaretheorem[numberlike=Theorem,style=mystyle]{Lemma}
\declaretheorem[numberlike=Theorem,style=mystyle]{Proposition}
\declaretheorem[numberlike=Theorem,style=mystyle]{Corollary}
\declaretheorem[numberlike=Theorem,style=mystyle]{Definition}
\declaretheorem[numberlike=Theorem,style=mystyle]{Assumption}
\declaretheorem[numberwithin=section,style=remark]{Remark}
% ------------------------------------------------------------------------

% ------------------------------------------------------------------------
% Handy commands and shortcuts

\newcommand{\mypar}[1]{\bigskip\noindent {\bf #1.}}
\newcommand\mynote[1]{\mbox{}\marginpar{\footnotesize\raggedright\hspace{0pt}\color{blue}\emph{#1}}}
%\newcommand{\qed}{\hfill $\Box$}

% Reference to Figures, Tables, etc
\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\eref}[1]{Equation~\ref{#1}}
\newcommand{\cref}[1]{Chapter~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\ssref}[1]{Subsection~\ref{#1}}
\newcommand{\aref}[1]{appendix~\ref{#1}}
%\newcommand{\assref}[1]{Assumption~\ref{#1}}
\newcommand{\alref}[1]{Algorithm~\ref{#1}}
\newcommand{\pref}[1]{Proposition~\ref{#1}}
\newcommand{\thref}[1]{Theorem~\ref{#1}}
\newcommand{\dref}[1]{Definition~\ref{#1}}
\newcommand{\lref}[1]{Lemma~\ref{#1}}
\newcommand{\coref}[1]{Corollary~\ref{#1}}
% ------------------------------------------------------------------------

% ------------------------------------------------------------------------
% Bibliography
\addbibresource{refs.bib}
% ------------------------------------------------------------------------


\title{ADMM for snapshot foreground-background separation}
\author{}	
\date{}
	
\begin{document}

\maketitle

\section{Problem formulation}

Let $X \in \mathbb{R}^{M\times N \times F}$ be a video sequence, of which
we observe
\begin{align}
  \label{eq:measurement}
  Y = \mathcal{H}(X)\,,
\end{align}
where $\mathcal{H}\,:\, \mathbb{R}^{M \times N \times F} \to \mathbb{R}^{M
\times Q}$ is a linear operator that models snapshot compressive imaging. When
$Q = N$, we obtain the traditional setup of
CACTI~\cite{Llull13-CodedApertureCompressiveTemporalImaging}. When $Q = N + F
-1$, we obtain the one-pixel (horizontal) shift scenario
of~\cite{Matin21-CompressiveCodedRotatingMirrorCameraForHighSpeedImaging}. 

\begin{Assumption}
  $X \in \mathbb{R}^{M \times N\times F}$ can be decomposed as
  \begin{align}
    \label{eq:assumptionlowrankplussparse}
    X = B + S\,,
  \end{align}
  where 
  \begin{itemize}
    \item $B \in \mathbb{R}^{M\times N\times F}$ represents a
      \textit{low-rank} background, in the sense that the matrix
      $\overline{B} \in \mathbb{R}^{MN \times F}$ is low-rank. Each column $f$
      of $\overline{B}$ is obtained by vectorizing the $f$th frame of
      $B$: $\overline{B}_{:, f} = \text{vec}(B_{:, :, f})$, for $f
      = 1,\ldots, F$.

    \item $S \in \mathbb{R}^{M\times N\times F}$ represents a sparse
    foreground, satisfying patch similarity. That is, many of the entries of
    $S$ are zero, and the matrix $\widetilde{S} \in \mathbb{R}^{P
    \times L}$ is low-rank. The $L$ columns of the matrix $\widetilde{S}$ are obtained by
    vectorizing $L$ patches of size $\sqrt{P}\times \sqrt{P}$ from the frames
    $S_f$, $f = 1, \ldots, F$. 
  \end{itemize}
\end{Assumption}

Notice that because $S$ is sparse, the matrix $\widetilde{S}$ only needs to
contain patches whose pixels are nonzero. This will reduce the number of
patches $L$. Throughout, given a 3D tensor $M \in \mathbb{R}^{d_1 \times d_2
\times d_{3}}$, the notation $\overline{M} \in \mathbb{R}^{d_1 d_2 \times d_3}$
represents the matrix whose columns are obtained by vectorizing each slice
$M_{:, :, i}$, along the third dimension of $M$. And $\widetilde{M} \in
\mathbb{R}^{P \times L}$ refers to the matrix obtained by extracting $L$
patches for size $\sqrt{P} \times \sqrt{P}$ from slices of $M_{:, :, i}$, for
any $i$, and vectorizing them into the columns of $\widetilde{M}$. 

\mypar{Optimization problem}
Given the above assumption, we can formulate the reconstruction problem as
\begin{align}
  \label{eq:prob}
  \underset{B, S}{\text{minimize}}\,\,\,
  \frac{1}{2}
  \big\|Y - \mathcal{H}(B + S)\big\|_F^2
  +
  \lambda_1 \big\|S\big\|_1
  +
  \lambda_2 \big\|\overline{B}\big\|_{\star}
  +
  \lambda_3 \big\|\widetilde{S}\big\|_{\star}\,,
\end{align}
where 
\begin{itemize}
  \item $\|\cdot\|_F$ is the Frobenius norm. It is the sum of squared entries,
    whether applied to a matrix or a tensor.

  \item $\|\cdot\|_1$ is the $\ell_1$-norm (sum of absolute values). When
    applied to a matrix or tensor, it is the same as the $\ell_1$-norm applied
    to their vectorization.

  \item $\|\cdot\|_{\star}$ is the nuclear-norm (Schatten $1$-norm), which is
    the $\ell_1$-norm applied to the singular values of the matrix.

  \item $\lambda_1, \lambda_2, \lambda_3 \geq 0$ are nonnegative
    hyperparameters.
\end{itemize}

\section{Applying ADMM} 

To apply ADMM to~\eqref{eq:prob}, we need to rewrite it in a way that the
variables become decoupled. To do so, we introduce two copies of 
$S$, denoted $U$ and $V$, and one copy of $B$, denoted $L$. 
Problem~\eqref{eq:prob} is thus equivalent to
\begin{align}
  \label{eq:reformulation}
  \begin{array}[t]{cl}
    \underset{S, U, V, B, L}{\text{minimize}}
    &
    \frac{1}{2}
    \big\|Y - \mathcal{H}(B + S)\big\|_F^2
    +
    \lambda_1 \big\|U\big\|_1
    +
    \lambda_2 \big\|\overline{L}\big\|_{\star}
    +
    \lambda_3 \big\|\widetilde{V}\big\|_{\star}
    \\
    \text{subject to}
    &
    S = U
    \\
    & S = V
    \\
    & 
    B = L\,.
  \end{array}
\end{align}
The augmented Lagrangian of~\eqref{eq:reformulation} is
\begin{multline*}
  L_{\rho}(S, U, V, B, L\, ;\, \Theta, \Gamma, \Delta) 
  = 
  \frac{1}{2}
  \big\|Y - \mathcal{H}(B + S)\big\|_F^2
  +
  \lambda_1 \big\|U\big\|_1
  +
  \lambda_2 \big\|\overline{L}\big\|_{\star}
  +
  \lambda_3 \big\|\widetilde{V}\big\|_{\star}
  \\
  +
  \langle\Theta, S - U\rangle
  +
  \langle\Gamma, S - V\rangle
  +
  \langle\Delta, B - L\rangle
  +
  \frac{\rho}{2}
  \|S - U\|_{F}^2
  +
  \frac{\rho}{2}
  \|S - V\|_{F}^2
  +
  \frac{\rho}{2}
  \|B - L\|_{F}^2\,,
\end{multline*}
where we associated the dual variables $\Theta, \Gamma, \Delta \in
\mathbb{R}^{M\times N\times F}$ with the 1st, 2nd, and 3rd constraints
of~\eqref{eq:reformulation}.

We group the primal variables into $(S, L)$ and $(U, V, B)$. ADMM iteratively
minimizes $L_\rho$ w.r.t.\ the first set of variables and, then, with these
variables fixed at the new value, minimizes $L_\rho$ w.r.t.\ the second set of
variables. Finally, it updates all the Lagrange multipliers.

More concretely, starting with arbitrary $\big(U^0, V^0, B^0\big)$ and
$\big(\Theta^0, \Gamma^0, \Delta^0\big)$, ADMM iterates for $k = 0, 1, \ldots$, 
\begin{subequations}
  \label{eq:admm_applied}
  \begin{align}
    (S^{k+1}, L^{k+1})
      &=
      \underset{S, L}{\arg\min}\,\,\,
      \bigg[
      \frac{1}{2}
      \big\|Y - \mathcal{H}(B^k + S)\big\|_F^2
      +
      \langle\Theta^k, S - U^k\rangle
      +
      \frac{\rho}{2}
      \|S - U^k\|_{F}^2
      +
      \frac{\rho}{2}
      \|S - V^k\|_{F}^2
      \bigg]
      \notag
      \\
      &\qquad\qquad\qquad\qquad\qquad
      +
      \bigg[
        \lambda_2 \big\|\overline{L}\big\|_{\star}
        +
        \langle\Delta^k,B^k - L\rangle
        +
        \frac{\rho}{2}
        \|B^k - L\|_{F}^2
      \bigg]
      \label{eq:admm_applied_sl}
      \\
      (U^{k+1}, V^{k+1}, B^{k+1})
      &=
      \underset{U, V, B}{\arg\min}\,\,\,
      \bigg[
        \lambda_1 \big\|U\big\|_1
        +
        \langle\Theta^k, S^{k+1} - U\rangle
        +
        \frac{\rho}{2}
        \|S^{k+1} - U\|_{F}^2
      \bigg]
      +
      \bigg[
        \lambda_3 \big\|\widetilde{V}\big\|_{\star}
        \notag
        \\
      &\qquad\qquad\qquad\qquad\qquad
        +
        \langle\Gamma^k, S^{k+1} - V\rangle
        +
        \frac{\rho}{2}
        \|S^{k+1} - V\|_{F}^2
      \bigg]
        \notag
        \\
      &\qquad\qquad
      +
      \bigg[
        \frac{1}{2}
        \big\|Y - \mathcal{H}(B + S^{k+1})\big\|_F^2
        +
        \langle\Delta^k, B - L^{k+1}\rangle
        +
        \frac{\rho}{2}
        \|B - L^{k+1}\|_{F}^2
      \bigg]
      \label{eq:admm_applied_uvb}
      \\
      \Theta^{k+1} &= \Theta^k +  \rho(S^{k+1} - U^{k+1})
      \label{eq:admm_applied_lagrangetheta}
      \\
      \Gamma^{k+1} &= \Gamma^k +  \rho(S^{k+1} - V^{k+1})
      \label{eq:admm_applied_lagrangegamma}
      \\
      \Delta^{k+1} &= \Delta^k + \rho(B^{k+1} - L^{k+1})\,.
      \label{eq:admm_applied_lagrangedelta}
  \end{align}
\end{subequations}
We can see that~\eqref{eq:admm_applied_sl} decomposes into two independent
problems, in $S$ and in $L$, that can be solved in parallel. Also,
\eqref{eq:admm_applied_uvb} decomposes into three independent problems, in $U$,
$V$, and $B$, that can be solved in parallel. We now elaborate on how to solve
each of these problems.

\mypar{Updating $\bm{S}$}
The problem in $S$ in~\eqref{eq:admm_applied_sl} can be rewritten as
\begin{align}
  &
  \underset{S}{\text{minimize}}\,\,\,
  \frac{1}{2}
  \big\|Y - \mathcal{H}(B^k + S)\big\|_F^2
  +
  \langle\Theta^k, S - U^k\rangle
  +
  \frac{\rho}{2}
  \big\|S - U^k\big\|_{F}^2
  +
  \frac{\rho}{2}
  \big\|S - V^k\big\|_{F}^2
  \notag
  \\[0.2cm]
  \Longleftrightarrow\qquad &
  \underset{S}{\text{minimize}}\,\,\,
  \frac{1}{2}
  \big\|(Y - \mathcal{H}(B^k)) - \mathcal{H}(S)\big\|_F^2
  +
  \rho
  \bigg\|
  S - \frac{1}{2}\Big((U^k + V^k) - \frac{1}{\rho}\Theta^k\Big)
  \bigg\|_F^2\,.
  \label{eq:probs1}
\end{align}
Defining
\begin{align*}
  Y_B &:= Y - \mathcal{H}(B^k) \in \mathbb{R}^{M\times Q}
      &
  S_a &:= \frac{1}{2}\Big((U^k + V^k) - \frac{1}{\rho}\Theta^k\Big) \in
  \mathbb{R}^{M \times N \times F}
  \\
  y_b &= \text{vec}(Y_B) \in \mathbb{R}^{MQ}
    &
  s_a &:= \text{vec}(S_a) \in \mathbb{R}^{MNF}\,,
\end{align*}
where the vectorization of a tensor follows a specific order (described below),
problem~\eqref{eq:probs1} can be written in a vector format:
\begin{align*}
  \underset{s \in \mathbb{R}^{MNF}}{\text{minimize}}\,\,\,
  \frac{1}{2}
  \|y_b - Hs\|_2^2
  +
  \rho
  \| s - s_a\|_2^2\,,
\end{align*}
where $H \in \mathbb{R}^{MQ \times MNF}$ is a matricial representation of
$\mathcal{H}$. Taking the gradient w.r.t.\ $s$ and equating it to zero, we
obtain a closed-form solution for updating $S$:
\begin{align}
  s^{k+1}
  =
  \left(H^\top H + 2\rho I_{MNF \times MNF}\right)^{-1}
  \left[H^\top y_b + 2\rho s_a\right]\,,
  \label{eq:probsclosedformvec}
\end{align}
where $I_{MNF \times MNF}$ is the identity matrix. 

In practice, we should not compute the matrices
in~\eqref{eq:probsclosedformvec} explicitly. To see how to perform the
operations in~\eqref{eq:probsclosedformvec}, let us consider the simple case of
conventional snapshot compressive imaging, i.e., no pixel-shifts. In this case,
$Y \in \mathbb{R}^{M\times N}$. Let $X \in \mathbb{R}^{M \times N\times F}$ be
a generic tensor and let $\mathcal{H}$ apply mask $M_f \in \mathbb{R}^{M\times
N}$ to frame $f$. In other words, 
\begin{align*}
  &\qquad
  Y = \sum_{f=1}^{F} M_f \odot X_{:,:,f} 
  \\
  \Longleftrightarrow &\qquad
  \text{vec}(Y)
  =
  \sum_{f=1}^{F} \text{vec}(M_f) \odot \text{vec}(X_{:,:,f})
  \\
  \Longleftrightarrow &\qquad
  \underbrace{\text{vec}(Y)}_{y}
  =
  \underbrace{
  \begin{bmatrix}
    \text{Diag}(m_1) & \cdots & \text{Diag}(m_F)
  \end{bmatrix}
  }_{H}
  \underbrace{
  \begin{bmatrix}
    \text{vec}(X_{:, :, 1})
    \\
    \vdots
    \\
    \text{vec}(X_{:, :, F})
  \end{bmatrix}}_{x}\,,
\end{align*}
where $\odot$ represents the pointwise (Hadamard) product, $m_f :=
\text{vec}(M_f)$, and $\text{Diag}(\cdot)$ represents a diagonal matrix with a
specified vector in the diagonal. The matrix $H$ is indicated as well as the
vectorization of the tensor $X$. We can now see that $H^\top H$
in~\eqref{eq:probsclosedformvec} will be a diagonal matrix:
\begin{align*}
  H^\top H = \text{Diag}\left(\begin{bmatrix}
    m_1^2 \\ \vdots  \\m_F^2
  \end{bmatrix}\right)\,,
\end{align*}
where $m_f^2 := m_f \odot m_f$ denotes the vector $m_f$ with squared entries.
Undoing the vectorization operations, we can compute the result
of~\eqref{eq:probsclosedformvec} from right to left according to
Algorithm~\ref{alg:updateS}. In step~\ref{subAlg:lastStep}, $\oslash$ denotes
the elementwise division, and $1_{M\times N}$ the matrix of all-ones in
$\mathbb{R}^{M\times N}$.

\begin{algorithm}
  \caption{Update of $S$ in~\eqref{eq:probsclosedformvec} for
  conventional SCI}
  \label{alg:updateS}
  \begin{algorithmic}[1]
  %\small
    \State $Y_B = Y - \mathcal{H}(B^k) \in \mathbb{R}^{M\times N}$
    \State $C_1 = \rho (U^k + V^k - \frac{1}{\rho}\Theta^k) \in \mathbb{R}^{M\times N\times F}$
    \State $C_2 \in \mathbb{R}^{M\times N\times F}$ such that each slice
    $(C_2)_{:, :, f} = M_f \odot Y_B$, $f = 1, \ldots, F$
    \State $C_3 = C_1 + C_2 \in \mathbb{R}^{M \times N\times F}$
    \State $S^{k+1} \in \mathbb{R}^{M\times N\times F}$ such that each slice
    $S^{k+1}_{:, :, f} = \left[1\oslash(M_f^2 + 2\rho 1_{M\times N})\right]
    \odot {C_3}_{:, :, f}$,
    $f=1, \ldots, F$
    \label{subAlg:lastStep}
  \end{algorithmic}
\end{algorithm}
This code can run fast if
\begin{itemize}
  \item Memory to store $Y_B, C_1, C_2, C_3, S^{k+1}$ is preallocated

  \item Operations in each step are parallelized, e.g., in a GPU.
\end{itemize}
The total operation count is roughly $13MNF$ FLOPs.

\mypar{Updating $\bm{L}$}
The problem in $L$ in~\eqref{eq:admm_applied_sl} can be rewritten as
\begin{align}
  &\qquad
  \underset{L}{\text{minimize}} \,\,\,
  \lambda_2 \big\|\overline{L}\big\|_{\star}
  +
  \langle\Delta^k,B^k - L\rangle
  +
  \frac{\rho}{2}
  \|B^k - L\|_{F}^2
  \notag
  \\
  \Longleftrightarrow
  &\qquad
  \underset{L}{\text{minimize}} \,\,\,
  \frac{\lambda_2}{\rho} \big\|\overline{L}\big\|_{\star}
  +
  \frac{1}{2}
  \Big\|L - \big(B^k + \frac{1}{\rho} \Delta^k\big)\Big\|_F^2\,.
  \label{eq:probL1}
\end{align}
In other words, 
\begin{align}
  \label{eq:proxL}
  L^{k+1} =
  \text{prox}_{\frac{\lambda_2}{\rho}\|\overline{\cdot}\|_{\star}}\Big(B^k +
  \frac{1}{\rho}\Delta^k\Big)\,,
\end{align}
where the \textit{proximal operator} of a function $f$ at point $y$ is
defined as
\begin{align*}
  \text{prox}_f(y)
  :=
  \underset{x}{\arg\min}\,\,\, f(x) + \frac{1}{2}\|x - y\|_2^2\,.
\end{align*}
If we use a simple nuclear norm (unweighted), the solution to~\eqref{eq:proxL}
requires
\begin{enumerate}
  \item Forming the tensor $L_a := B^k + \frac{1}{\rho}\Delta^k \in
    \mathbb{R}^{M\times N\times F}$ and reshaping it to a matrix
    $\overline{L}_a \in \mathbb{R}^{MN \times F}$.

  \item Computing the singular value decomposition of $\overline{L}_a$:
    \begin{align*}
      \overline{L}_a = U \Sigma V^\top\,,
    \end{align*}
    where $U \in \mathbb{R}^{MN \times k}$ and $V \in \mathbb{R}^{F \times k}$
    contain orthonormal columns, and $\Sigma \in \mathbb{R}^{k \times k}$ is a
    diagonal matrix with singular values. The integer $k$ is the rank of
    $\overline{L}_a$ and is usually not known in advance.

  \item Constructing the solution $\overline{L}^{k+1} = U
    \mathcal{S}_{\frac{\lambda_2}{\rho}}(\Sigma)V^\top$\,, where
    $\mathcal{S}_{c}(x) := \max\{x - c, 0\}$ is a soft-thresholding operation
    applied componentwise to a matrix. See, e.g.,
    \cite[(1)-(3)]{Gu17-WeightedNuclearNormMinimizationAndItsApplicationsToLowLevelVision}.
    Then, we need to reshape $\overline{L}^{k+1}$ back to a tensor $L^{k+1}$.
\end{enumerate}
If we use a weighted nuclear
norm~\cite{Gu17-WeightedNuclearNormMinimizationAndItsApplicationsToLowLevelVision},
only step 3 will be different (see below). To compute the SVD decomposition in
step 2, we can use the randomized algorithms described
in~\cite{Halko11-FindingStructureWithRandomness}.

In Python, the following packages implement algorithms
from~\cite{Halko11-FindingStructureWithRandomness}:
\begin{itemize}
  \item \href{https://docs.pytorch.org/docs/stable/generated/torch.svd_lowrank.html}{PyTorch}

  \item \href{https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_svd.html#rf38c2b656ebc-1}{Scikit-learn}
\end{itemize}
According to~\cite[\S4.6, \S5.1]{Halko11-FindingStructureWithRandomness}, the
cost of computing the SVD decomposition in step 2 should be $O(MNF(\log(\ell) +
k))$, where $k$ is the rank of $\overline{L}_a$ and $\ell$ is the oversampling parameter,
which is typically a small number (between $2$ and $10$).
This assumes the computation is done using the FFT rather than Gaussian
matrices.

\begin{Remark}
An important aspect in terms of implementation is that if the above software
packages accept implicit access to $\overline{L}_a$, i.e., rather than the full
matrix, only functions performing matrix vector products $\overline{L}_a x$ and
$\overline{L}_a ^\top y$, we may be able to compute the SVD decomposition
without reshaping $L_a$ into $\overline{L}_a$.
\end{Remark}

According
to~\cite{Gu17-WeightedNuclearNormMinimizationAndItsApplicationsToLowLevelVision,Liu19-RankMinimizationForSnapshotCompressiveImaging}, we can
improve the performance of the algorithm by using a weighted (or reweighted)
nuclear norm. That is, instead of~\eqref{eq:probL1}, we solve
\begin{align}
  \underset{\overline{L}}{\text{minimize}} \,\,\,
  \frac{\lambda_2}{\rho} \big\|\overline{L}\big\|_{w,\star}
  +
  \frac{1}{2}
  \big\|\overline{L} - \overline{L}_a\big\|_F^2\,,
  \label{eq:proxLnew}
\end{align}
where $\overline{L}_a$ is defined in step 1 above, 
\begin{align}
  \label{eq:weightednuclearnorm}
  \|X\|_{w, \star} := \sum_{i} w_i \sigma_i(X)\,,
\end{align}
is the \textit{weighted nuclear norm}, and $\sigma_i(X)$ represents the $i$th
singular value of $X$. We assume $w_i \geq 0$, for all $i$. Theorem 1
of~\cite{Gu17-WeightedNuclearNormMinimizationAndItsApplicationsToLowLevelVision}
applies the von Neumann trace inequality to show that the solution
of~\eqref{eq:proxLnew} is given by
\begin{align*}
  \overline{L}^{k+1}
  =
  U D V^\top\,,
\end{align*}
where $\overline{L}_a = U \Sigma V^\top$ is the SVD decomposition of
$\overline{L}_a$, and $D = \text{Diag}\big(d_1, \ldots, d_{\min\{MN, F\}}\big)$ is a
diagonal matrix whose diagonal values are the solution of 
\begin{align}
  \label{eq:proxweightednucnorm}
  \begin{array}[t]{cl}
    \underset{d_1, \ldots, d_{\min\{MN, F\}}}{\text{minimize}}
    &
    \sum_{i=1}^{\min\{MN, F\}} 
    \frac{1}{2}(\sigma_i - d_i)^2 + \frac{\lambda_2 w_i}{\rho} d_i
    \\
    \text{subject to}
    &
    d_1 \geq d_2 \geq \cdots \geq d_n \geq 0\,,
  \end{array}
\end{align}
where $\sigma_i$ is the $i$th diagonal entry of $\Sigma$.
As in Corollary 1
of~\cite{Gu17-WeightedNuclearNormMinimizationAndItsApplicationsToLowLevelVision},
if the weights satisfy $0\leq w_1 \leq w_2 \leq \cdots \leq w_{\min\{MN, F\}}$,
then the constraint of~\eqref{eq:proxweightednucnorm} is automatically
satisfied, and its solution is given by
\begin{align}
  \label{eq:proxweightednucnormWithDecWeights}
  d_i = \max\Big\{\sigma_i - \frac{\lambda_2}{\rho}w_i\,, 0\Big\}\,,\qquad\quad
  i = 1, \ldots, \min\{MN, F\}\,.
\end{align}
The paper suggests a reweighted solution based on reweighted $\ell_1$-norm
minimization~\cite{Candes08-EnhancingSparsityReweightedL1Minimization}, where
weights are iteratively updated as
\begin{align}
  \label{eq:probLReweightedupdate}
  w_i^{t+1} = \frac{1}{d_i^t + \epsilon}\,,\qquad
  i = 1, \ldots, \min\{MN, F\}\,,
\end{align}
where $t$ denotes the iteration number, $d_i^t$ is the estimate of
the $i$th singular value of $\overline{L}^{k+1}$ at iteration $t$, and $\epsilon > 0$ is a small number for
stability.
The weights in~\eqref{eq:probLReweightedupdate} automatically satisfy the
ordering required for~\eqref{eq:proxweightednucnormWithDecWeights}. 
The authors
in~\cite{Gu17-WeightedNuclearNormMinimizationAndItsApplicationsToLowLevelVision}
compute the limit point of these iterations in closed-form, but the analysis
looks complicated and the solution seems to have one more free parameter than
necessary. 

My suggestion is to implement the iterations
in~\eqref{eq:probLReweightedupdate} recursively using, for example $\epsilon =
10^{-3}$. In my experience, these requires from $3$ to $40$ iterations to
converge. The resulting algorithm is described in Algorithm~\ref{alg:updateL}.
Note that Algorithms~\ref{alg:updateS} and~\ref{alg:updateL} can be run in
parallel.

\begin{algorithm}
  \caption{Update of $L$ in~\eqref{eq:proxLnew}}
  \label{alg:updateL}
  \begin{algorithmic}[1]
  %\small
  \algrenewcommand\algorithmicrequire{\textbf{Initialization:}}
  \Require Stopping tolerance $\delta > 0$ (e.g., $10^{-3}$), stability number $\epsilon > 0$ (e.g., $10^{-3}$), maximum number of
  reweighting iterations $T > 0$ (e.g., $50$), parameters to compute the
  randomized SVD ($\ell$ and a small tolerance).
  \State Compute $L_a := B^k + \frac{1}{\rho}\Delta^k \in \mathbb{R}^{M\times N\times F}$
  \State Reshape $L_a$ into $\overline{L}_a \in \mathbb{R}^{MN \times F}$
  \State Compute SVD $\overline{L}_a = U \Sigma V^\top$ with randomized
  algorithm
  \Comment{Do not confuse $U, V$ with $U^k, V^k$}
  \State Initialize $d_i = \sigma_i$, for $i = 1, \ldots, \min\{MN, F\}$, where
  $\sigma_i$ is the $i$th diagonal entry of $\Sigma$.
  \For{$t=1, \ldots, T$}
    \State Compute, for all $i = 1, \ldots, \min\{MN, F\}$, 
    \Comment{Use vector form, not a for loop}
    $$
    d_i^{t+1} = \max\Big\{\sigma_i - \frac{\lambda_2}{\rho}\frac{1}{d_i^t + \epsilon}\, ,\, 0\Big\}
    $$
    \If{$\max_i |d_i^{t+1} - d_i^t| \leq \delta$}
      \State Break
    \EndIf
  \EndFor
  \State Compute matricial solution $\overline{L}^{k+1} = U
  \text{Diag}(d_1^{t+1}, \ldots, d_{\min\{MN, F\}}^{t+1}) V^\top$
  \State Reshape $\overline{L}^{k+1} \in \mathbb{R}^{MN \times F}$ into
  $L^{k+1} \in \mathbb{R}^{M\times N\times F}$
  \end{algorithmic}
\end{algorithm}

\mypar{Updating $\bm{U}$}
The problem in $U$ in~\eqref{eq:admm_applied_uvb} can be written as
\begin{align*}
  U^{k+1}
  &=
  \underset{U}{\arg\min}\,\,\,
  \lambda_1 \big\|U\big\|_1
  +
  \langle\Theta^k, S^{k+1} - U\rangle
  +
  \frac{\rho}{2}
  \|S^{k+1} - U\|_{F}^2
  \\
  &=
  \underset{U}{\arg\min}\,\,\,
  \frac{\lambda_1}{\rho} \big\|U\big\|_1
  +
  \frac{1}{2}
  \Big\|U - \big(S^{k+1} + \frac{1}{\rho}\Theta^k\big)\Big\|_F^2
  \\
  &=
  \text{prox}_{\frac{\lambda_1}{\rho}\|\cdot\|_1}\big(S^{k+1} +
  \frac{1}{\rho}\Theta^k\big)\,.
\end{align*}
The procedure to update $U$ is then quite simple: soft-thresholding all the
components of $U_a := S^{k+1} + \frac{1}{\rho}\Theta^k$ in parallel. This is
done in Algorithm~\ref{alg:updateU}.
\begin{algorithm}
  \caption{Update of $U$ in~\eqref{eq:admm_applied_uvb}}
  \label{alg:updateU}
  \begin{algorithmic}[1]
  %\small
    \State Compute $U_a = S^{k+1} + \frac{1}{\rho}\Theta^k$
    \For{$i, j, l$ in parallel}
      \State
      \begin{align*}
        U_{i j k}^{k+1}
        =
        \left\{
        \begin{array}{ll}
          (U_a)_{ijl} + \frac{\lambda}{\rho} &,\,\text{if $(U_a)_{ijl} > -\frac{\lambda}{\rho}$}
          \\[0.2cm]
          0 &,\,\text{if $\big|(U_a)_{ijl}\big| \leq \frac{\lambda}{\rho}$}
          \\[0.2cm]
          (U_a)_{ijl} - \frac{\lambda}{\rho} &,\,\text{if $(U_a)_{ijl} < \frac{\lambda}{\rho}$}
        \end{array}
        \right.
      \end{align*}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\mypar{Updating $\bm{V}$}
To update $V$ in~\eqref{eq:admm_applied_uvb}, we rewrite the corresponding
problem as
\begin{align*}
  V^{k+1}
  &=
  \underset{V}{\arg\min}\,\,\,
  \lambda_3 \big\|\widetilde{V}\big\|_{\star}
  +
  \langle\Gamma^k, S^{k+1} - V\rangle
  +
  \frac{\rho}{2}
  \|S^{k+1} - V\|_{F}^2
  \\
  &=
  \underset{V}{\arg\min}\,\,\,
  \frac{\lambda_3}{\rho} \big\|\widetilde{V}\big\|_{\star}
  +
  \frac{1}{2}
  \Big\|V - \big(S^{k+1} + \frac{1}{\rho}\Gamma^k\big)\Big\|_F^2\,,
\end{align*}
and notice that it has exactly the same format as the update of $L$
in~\eqref{eq:probL1}. The reshaping of the tensors, however, is different.
Apart from this, the procedure is exactly like the update of $L$ in
Algorithm~\ref{alg:updateL} and is described in Algorithm~\ref{alg:updateV}.
\begin{algorithm}
  \caption{Update of $V$ in~\eqref{eq:admm_applied_uvb}}
  \label{alg:updateV}
  \begin{algorithmic}[1]
  %\small
  \algrenewcommand\algorithmicrequire{\textbf{Initialization:}}
  \Require Stopping tolerance $\delta > 0$ (e.g., $10^{-3}$), stability number $\epsilon > 0$ (e.g., $10^{-3}$), maximum number of
  reweighting iterations $T > 0$ (e.g., $50$), parameters to compute the
  randomized SVD ($\ell$ and a small tolerance), patch size $P$.
  \State Compute $V_a := S^{k+1} + \frac{1}{\rho}\Gamma^k \in \mathbb{R}^{M\times N\times F}$
  \State Reshape $V_a$ into $\widetilde{V}_a \in \mathbb{R}^{P \times L}$ by
  extracting $L$ patches of size $\sqrt{P} \times \sqrt{P}$ from $V_a$ and vectorizing
  them into the columns of $\widetilde{V}_a$
  \Comment{See comment about sparsity}
  \label{subAlg:updateVPatchExt}
  \State Compute SVD $\widetilde{V}_a = U \Sigma V^\top$ with randomized
  algorithm
  \Comment{Do not confuse $U, V$ with $U^k, V^k$}
  \State Initialize $d_i = \sigma_i$, for $i = 1, \ldots, \min\{P, L\}$, where
  $\sigma_i$ is the $i$th diagonal entry of $\Sigma$.
  \For{$t=1, \ldots, T$}
    \State Compute, for all $i = 1, \ldots, \min\{P, L\}$, 
    \Comment{Use vector form, not a for loop}
    $$
    d_i^{t+1} = \max\Big\{\sigma_i - \frac{\lambda_3}{\rho}\frac{1}{d_i^t + \epsilon}\, ,\, 0\Big\}
    $$
    \If{$\max_i |d_i^{t+1} - d_i^t| \leq \delta$}
      \State Break
    \EndIf
  \EndFor
  \State Compute matricial solution $\widetilde{V}^{k+1} = U
  \text{Diag}(d_1^{t+1}, \ldots, d_{\min\{P, L\}}^{t+1}) V^\top$
  \State Reshape $\widetilde{V}^{k+1} \in \mathbb{R}^{P \times L}$ into
  $V^{k+1} \in \mathbb{R}^{M\times N\times F}$
  \label{subAlg:updateVReshapeBack}
  \end{algorithmic}
\end{algorithm}

This update is likely to be the most time-consuming step of the entire
algorithm, due to the potentially very large number of patches. One key point
of step \ref{subAlg:updateVPatchExt},
however, is that $S^{k+1}$ is likely to be sparse. This means $V_a$ is also
likely to be sparse. So the patch extraction in step
\ref{subAlg:updateVPatchExt} \textbf{\textit{should consider patches containing only nonzero
entries}}. Note:
\begin{itemize}
  \item We need to keep track of the location of the patches to be able to
    execute step~\ref{subAlg:updateVReshapeBack}.

  \item Algorithms~\ref{alg:updateL} and~\ref{alg:updateV} have similar code,
    so make sure to reuse it.
\end{itemize}

\mypar{Updating $\bm{B}$}
Finally, $B$ in~\eqref{eq:admm_applied_uvb} is updated as
\begin{align*}
  B^{k+1}
  &=
  \underset{B}{\arg\min}\,\,\,
  \frac{1}{2}
  \big\|Y - \mathcal{H}(B + S^{k+1})\big\|_F^2
  +
  \langle\Delta^k, B - L^{k+1}\rangle
  +
  \frac{\rho}{2}
  \|B - L^{k+1}\|_{F}^2
  \\
  &=
  \underset{B}{\arg\min}\,\,\,
  \frac{1}{2}
  \big\|\big(Y - \mathcal{H}(S^{k+1})\big) - \mathcal{H}(B)\big\|_F^2
  +
  \frac{\rho}{2}\Big\|B - \big(L^{k+1} -
  \frac{1}{\rho}\Delta^k\big)\Big\|_F^2\,,
\end{align*}
which has the same format as~\eqref{eq:probs1}, but note the difference in the
coefficient multiplying the last term.
Defining
\begin{align*}
  Y_S &:= Y - \mathcal{H}(S^{k+1}) \in \mathbb{R}^{M\times Q}
      &
  B_a &:= L^{k+1} - \frac{1}{\rho}\Delta^k \in \mathbb{R}^{M \times N \times F}
  \\
  y_s &= \text{vec}(Y_S) \in \mathbb{R}^{MQ}
    &
  b_a &:= \text{vec}(B_a) \in \mathbb{R}^{MNF}\,,
\end{align*}
a vectorized version of $b^{k+1}$ can be updated as
\begin{align*}
  b^{k+1}
  &=
  \underset{b \in \mathbb{R}^{MNF}}{\arg\min}\,\,\,
  \frac{1}{2}\|y_s - H b\|_2^2 
  + 
  \frac{\rho}{2}\|b - b_a\|_2^2
  \\
  &=
  \left(H^\top H + \rho I_{MNF \times MNF}\right)^{-1}
  \left[H^\top y_s + \rho b_a\right]\,,
\end{align*}
which has the same format as~\eqref{eq:probsclosedformvec}.
Just like Algorithm~\ref{alg:updateS}, 
Algorithm~\ref{alg:updateB} describes the update of $B$ for a conventional SCI
model (zero-pixel shift).
\begin{algorithm}
  \caption{Update of $B$ in~\eqref{eq:admm_applied_uvb} for conventional SCI}
  \label{alg:updateB}
  \begin{algorithmic}[1]
  %\small
    \State $Y_S = Y - \mathcal{H}(S^{k+1}) \in \mathbb{R}^{M\times N}$
    \State $C_1 = \rho (L^{k+1} - \frac{1}{\rho}\Delta^k) \in \mathbb{R}^{M\times N\times F}$
    \State $C_2 \in \mathbb{R}^{M\times N\times F}$ such that each slice
    $(C_2)_{:, :, f} = M_f \odot Y_S$, $f = 1, \ldots, F$
    \State $C_3 = C_1 + C_2 \in \mathbb{R}^{M \times N\times F}$
    \State $B^{k+1} \in \mathbb{R}^{M\times N\times F}$ such that each slice
    $B^{k+1}_{:, :, f} = \left[1\oslash(M_f^2 + \rho 1_{M\times N})\right] \odot C_3$,
    $f=1, \ldots, F$
  \end{algorithmic}
\end{algorithm}

\mypar{Updating $\bm{\Theta}, \bm{\Gamma}$, and $\bm{\Delta}$}
The updates of the dual variables
in~\eqref{eq:admm_applied_lagrangetheta}-\eqref{eq:admm_applied_lagrangedelta}
are trivial. We can, however, make a small simplification: notice how all
the dual variables in the above problems appear always multiplied by $1/\rho$.
We can therefore define 
\begin{align*}
  {\Theta'}^k &= \frac{1}{\rho}\Theta^k
  \\
  {\Gamma'}^k &= \frac{1}{\rho}\Gamma^k
  \\
  {\Delta'}^k &= \frac{1}{\rho}\Delta^k\,.
\end{align*}
Dividing both sides of
equations~\eqref{eq:admm_applied_lagrangetheta}-\eqref{eq:admm_applied_lagrangedelta}
by $\rho$, we obtain the updates in terms of these new variables:
\begin{align*}
  {\Theta'}^{k+1} &= {\Theta'}^k +  S^{k+1} - U^{k+1}
  \\
  {\Gamma'}^{k+1} &= {\Gamma'}^k +  S^{k+1} - V^{k+1}
  \\
  {\Delta'}^{k+1} &= {\Delta'}^k + B^{k+1} - L^{k+1}\,.
\end{align*}
This is equivalent to ADMM in scaled form~\cite[\S3.1.1]{Boyd11-ADMM}.

\mypar{Full algorithm implementation}
An implementation of the full algorithm should consider a stopping criterion in
terms of the primal and dual residuals~\cite[\S3.3]{Boyd11-ADMM} and adjust $\rho$ at every
iteration~\cite[\S3.4.1]{Boyd11-ADMM}. The primal residual at iteration $k$
should be
\begin{align*}
  r^{k+1}
  =
  \begin{bmatrix}
    S^{k+1} - U^{k+1}
    \\
    S^{k+1} - V^{k+1}
    \\
    B^{k+1} - L^{k+1}
  \end{bmatrix}\,,
\end{align*}
and the dual residual should be
\begin{align*}
  s^{k+1}
  =
  -\rho
  \begin{bmatrix}
    U^{k+1} - U^k + V^{k+1} - V^k
    \\
    B^{k+1} - B^k
  \end{bmatrix}\,.
\end{align*}
See
\href{https://github.com/marijavella/sr-via-CNNs-and-tvtv/blob/master/code/TVTV_Solver_CPU.m}{Github}
for an example of the implementation of ADMM to solve a simpler problem.


\printbibliography

\end{document}


